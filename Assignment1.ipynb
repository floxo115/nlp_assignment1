{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">344.075 KV: Natural Language Processing (WS2021/22)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 1</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Document Classification with Standard Machine Learning Methods</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University (JKU) Linz, and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Author:** Navid Rekab-Saz<br>\n",
    "**Email:** navid.rekabsaz@jku.at<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-preprocessing\"><li style=\"font-size:large;font-weight:bold\">Task A: Pre-processing & Feature Extraction (15 points)</li></a>\n",
    "    <a href=\"#section-training\"><li style=\"font-size:large;font-weight:bold\">Task B: Training and Results Analysis (15 points)</li></a>\n",
    "    <a href=\"#section-optional\"><li style=\"font-size:large;font-weight:bold\">Task C: Linear Model Interpretability (2 extra point)</li></a>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "\n",
    "### Assignment objective\n",
    "\n",
    "The aim of this assignment is to implement a document (sentence) classification model using (standard) machine learning methods. The assignment in total has **30 points**; it also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n",
    "\n",
    "### Implementation & Libraries\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python` (>3.7). Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Each group submits one Notebook file (`.ipynb`) trough MOODLE. Do not forget to put in your names and student numbers in the first cell of the Notebook. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that one can run all the cells from top to bottom without any error. If you need to include extra files in the submission, compress all files (together with the Notebook) in a `zip` file and submit the zip file to MOODLE. You do not need to include the data files in the submission.\n",
    "\n",
    "Cover the questions/points, mentioned in the tasks, but also add any necessary point for understanding your experiments.  \n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "To conduct the experiments, two datasets are provided. The datasets are taken from the data of `thedeep` project, produced by the DEEP (https://www.thedeep.io) platform. The DEEP is an open-source platform, which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset has 12 classes (labels) like agriculture, health, and protection. \n",
    "\n",
    "The difference between the datasets is in their sizes. We refer to these as `medium` and `small`, containing an overall number of 38,000 and 12,000 annotated text excerpts, respectively. Select one of the datasets, and use it for all of the tasks. `medium` provides more data and therefore reflects a more realistic scenario. `small` is however provided for the sake of convenience, particularly if running the experiments on your available hardware takes too long. Using `medium` is generally recommended, but from the point of view of assignment grading, there is no difference between the datasets.\n",
    "\n",
    "Download the dataset from [this link](https://drive.jku.at/filr/public-link/file-download/0cce88f07c9c862b017c9cfba294077a/33590/5792942781153185740/nlp2021_22_data.zip).\n",
    "\n",
    "Whether `medium` or `small`, you will find the following files in the provided zip file:\n",
    "- `thedeep.$name$.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.label.txt`: Captions of the labels.\n",
    "- `README.txt`: Terms of use of the dataset.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-preprocessing\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Pre-processing & Feature Extraction (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    \n",
    "**Preprocessing (4 points).** Load the train, validation, and test sets. Study the text and according to your judgements, apply at least <ins>two text cleaning/preprocessing methods</ins>. Punctuations marks, numbers, dates, case-sensitivity are some examples of the elements which can be potentially considered for cleaning/preprocessing. Tokenize the result text with a tokenizer of your choice. Report your approaches to text cleaning and tokenization and the reasons of your choices. Provide some examples, showing the effects of the applied approaches on the text.\n",
    "\n",
    "**Creating dictionary (4 points).** Create a dictionary of vocabularies following the guidelines discussed in the lecture. Next, reduce the size of dictionary using a method of your choice, for instance by considering a cut-off threshold on the tokens with low frequencies. When removing tokens from the dictionary, consider a strategy for handling Out-Of-Vocabulary (OOV) tokens, namely the ones in the train/validation/test datasets that that are not anymore in the dictionary. Some possible strategies could be to remove OOVs completely from the texts, or to replace them with a special token like <OOV\\>. Explain your approaches and report the statistics of the dictionary before and after the reduction.\n",
    "\n",
    "**Creating sentence vectors (4 points).** Use the dictionary to prepare <ins>two variations of document representation vectors</ins>, separately for train, validation, and test sets. Both variations follow a Bag-of-Words approach with a different token weighting method. The applied weightings can be any of the methods discussed in the lecture (namely `tc`, `tf`, and `tf-idf`), or other possible methods of your choice. Report the applied approaches. Calculate and report the sparsity rate of the vectors of train, validation, and test sets, namely what percentages of the vectors in each set are filled with zeros.\n",
    "\n",
    "**Dimensionality reduction (3 points).** Reduce vectors' dimensions to $k$ by applying Latent Semantic Analysis (LSA) to the vectors of both variations. $k$ is a hyper-parameter and can be $10<k<1000$. Keep in mind the training and inference phases of LSA, when applied to the train, validation, and test sets. \n",
    "\n",
    "At the end of Task A, you should have the <ins>four feature vectors variations</ins> shown below, each consisting of the sets of train, validation, and test:\n",
    "- **`Token Weighting I - High Dimensional`**\n",
    "- **`Token Weighting I - Low Dimensional`**\n",
    "- **`Token Weighting II - High Dimensional`**\n",
    "- **`Token Weighting II - Low Dimensional`**\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import import_datasets, get_n_most_common_tokens, preprocess_txt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import dill\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"small\" # \"medium\"\n",
    "# if already preprocessed then set to false to save time\n",
    "PREPROC = False\n",
    "REDUCE_WORDS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 29747),\n",
      " ('of', 24137),\n",
      " ('and', 22190),\n",
      " ('in', 19088),\n",
      " ('to', 17114),\n",
      " ('are', 6384),\n",
      " ('a', 6195),\n",
      " ('is', 4828),\n",
      " ('for', 4671),\n",
      " ('have', 4605),\n",
      " ('The', 4177),\n",
      " ('as', 3615),\n",
      " ('by', 3584),\n",
      " ('from', 3547),\n",
      " ('that', 3461),\n",
      " ('with', 3387),\n",
      " ('on', 3356),\n",
      " ('were', 3315),\n",
      " ('food', 2969),\n",
      " ('people', 2896)]\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, val_df = import_datasets(DATASET_SIZE)\n",
    "pprint(get_n_most_common_tokens(train_df, 20))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROC:\n",
    "    preprocess_txt(train_df)\n",
    "    preprocess_txt(test_df)\n",
    "    preprocess_txt(val_df)\n",
    "\n",
    "    with open(\"train_df_preprocessed.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(train_df))\n",
    "    with open(\"test_df_preprocessed.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(test_df))\n",
    "    with open(\"val_df_preprocessed.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(val_df))\n",
    "\n",
    "else:\n",
    "    with open(\"train_df_preprocessed.pkl\", \"rb\") as f:\n",
    "            train_df = dill.loads(f.read())\n",
    "    with open(\"test_df_preprocessed.pkl\", \"rb\") as f:\n",
    "            test_df = dill.loads(f.read())\n",
    "    with open(\"val_df_preprocessed.pkl\", \"rb\") as f:\n",
    "            val_df = dill.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 3809),\n",
      " ('peopl', 3414),\n",
      " ('report', 3231),\n",
      " ('case', 2695),\n",
      " ('area', 2637),\n",
      " ('water', 2531),\n",
      " ('children', 2212),\n",
      " ('need', 2039),\n",
      " ('health', 2019),\n",
      " ('affect', 1964),\n",
      " ('access', 1687),\n",
      " ('includ', 1612),\n",
      " ('displac', 1585),\n",
      " ('increas', 1434),\n",
      " ('due', 1374),\n",
      " ('also', 1347),\n",
      " ('refuge', 1324),\n",
      " ('school', 1299),\n",
      " ('continu', 1293),\n",
      " ('region', 1290)]\n"
     ]
    }
   ],
   "source": [
    "pprint(get_n_most_common_tokens(train_df, 20))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter = Counter((\" \".join(train_df[\"text\"].values).split(\" \")))\n",
    "token_counter.pop(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19074"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_dict = pd.DataFrame({\"token\": token_counter.keys(), \"count\": token_counter.values()})\n",
    "corpus_dict = corpus_dict.sample(frac=1).reset_index(drop=True)\n",
    "sorted_dict = corpus_dict.sort_values(\"count\", ascending=False)\n",
    "\n",
    "plt.plot(range(1, len(sorted_dict) + 1), sorted_dict[\"count\"])\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduced_corpus_dict = corpus_dict[corpus_dict[\"count\"] >= 100]\n",
    "\n",
    "sorted_dict = reduced_corpus_dict.sort_values(\"count\", ascending=False)\n",
    "plt.plot(range(1, len(sorted_dict) + 1), sorted_dict[\"count\"])\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_words(input_df: pd.DataFrame, full_dict: pd.DataFrame, threshold: int):\n",
    "    def remove_words(word_list):\n",
    "        def inner(text):\n",
    "            text = \" \".join([w for w in text.split(\" \") if w not in word_list])\n",
    "            return text\n",
    "        return inner\n",
    "\n",
    "    word_list = (full_dict[full_dict[\"count\"] < threshold])[\"token\"].to_list()\n",
    "    input_df[\"text\"] = input_df[\"text\"].apply(remove_words(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REDUCE_WORDS:\n",
    "    threshold = 100\n",
    "    reduce_words(train_df, corpus_dict, threshold)\n",
    "    reduce_words(test_df, corpus_dict, threshold)\n",
    "    reduce_words(val_df, corpus_dict, threshold)\n",
    "\n",
    "    with open(\"train_df_reduced.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(train_df))\n",
    "    with open(\"test_df_reduced.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(test_df))\n",
    "    with open(\"val_df_reduced.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(val_df))\n",
    "\n",
    "else:\n",
    "    with open(\"train_df_reduced.pkl\", \"rb\") as f:\n",
    "            train_df = dill.loads(f.read())\n",
    "    with open(\"test_df_reduced.pkl\", \"rb\") as f:\n",
    "            test_df = dill.loads(f.read())\n",
    "    with open(\"val_df_reduced.pkl\", \"rb\") as f:\n",
    "            val_df = dill.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = reduced_corpus_dict\n",
    "corpus_dict.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.69314718\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.69314718 0.         0.69314718\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.69314718 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.69314718 0.\n",
      " 0.         0.         0.         0.         0.         1.09861229\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.69314718 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.38629436 0.         0.         0.         0.69314718\n",
      " 0.         0.         0.         0.         0.         0.69314718\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.69314718 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         2.30258509 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.69314718 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.94591015 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.09861229 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.69314718 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.09861229 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.69314718 0.         0.\n",
      " 0.         0.69314718 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.09861229 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.69314718 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.69314718\n",
      " 0.         0.         0.         0.         0.69314718 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "def create_tc_vec_from_doc(input_df, n):\n",
    "    word_tokens = input_df.iloc[n][\"text\"]\n",
    "    tokens = word_tokenize(word_tokens)\n",
    "    fd = FreqDist(tokens)\n",
    "\n",
    "    doc_vec = np.zeros(len(corpus_dict))\n",
    "\n",
    "    tokens = corpus_dict[\"token\"]\n",
    "    for i, token in enumerate(tokens):\n",
    "        doc_vec[i] = fd[token]\n",
    "    \n",
    "    return doc_vec\n",
    "\n",
    "\n",
    "train_tc_vecs = []\n",
    "train_tf_vecs = []\n",
    "for doc_idx in range(len(train_df)):\n",
    "    train_tc_vecs.append(create_tc_vec_from_doc(train_df, doc_idx))\n",
    "    train_tf_vecs.append(np.log(train_tc_vecs[-1] + 1))\n",
    "\n",
    "test_tc_vecs = []\n",
    "test_tf_vecs = []\n",
    "for doc_idx in range(len(test_df)):\n",
    "    test_tc_vecs.append(create_tc_vec_from_doc(test_df, doc_idx))\n",
    "    test_tf_vecs.append(np.log(test_tc_vecs[-1] + 1))\n",
    "    \n",
    "val_tc_vecs = []\n",
    "val_tf_vecs = []\n",
    "for doc_idx in range(len(val_df)):\n",
    "    val_tc_vecs.append(create_tc_vec_from_doc(val_df, doc_idx))\n",
    "    val_tf_vecs.append(np.log(val_tc_vecs[-1] + 1))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(train_tc_vecs[0])\n",
    "print(train_tf_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-training\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Training and Results Analysis (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "\n",
    "To evaluate the models, use <ins>accuracy</ins> as the metric throughout the task. \n",
    "\n",
    "**Dummy baseline (2 points).** Create one dummy baseline classifier that predicts the validation/test labels only based on the distribution of the labels in the training set (without any use of the feature vectors). This is a weak baseline and acts as a sanity check for the actual classifiers.\n",
    "\n",
    "**Training and tuning classifiers (5 points).** Select at least <ins>two classification algorithms</ins> from standard machine learning classifiers. Using each classification algorithm, train a machine learning model on each of the variations of feature vectors. This should result in <ins>eight experiment sets</ins> (4 variations of feature vectors × 2 classification algorithms). The ML model in each of the experiments possibly have several involving hyper-parameters (also keep in mind the dimensionality size $k$ in the low-dimensional vectors as yet another hyper-parameter). For each experiment, select <ins>one of the hyper-parameters and tune its value</ins>. The tuning process is done by first assigning at least <ins>five different values</ins> to the hyper-parameter, then training separate models based on each value, and finally using the evaluation results on the validation set to select the best-performing model. Report the studied hyper-parameters, the evaluation results of each on validation set, and finally the selected value of the hyper-parameter. \n",
    "\n",
    "**Evaluation and plots (3 point).** Evaluate the selected model of the eight experiment on the test set and report the results of the experiments on <ins>both validation and test sets (side by side) in one table as well as in one plot</ins>. Compare different models. Are the test results lower(/higher) than the validation results? If it is the case, where can it be rooted from? Among all these models and variations, what are the most important factors improving the classification results?\n",
    "\n",
    "**Confusion matrix (2 point).** Select the best performing model among the experiments and use it to create a confusion matrix. The matrix shows the predicted versus true results per each label. Explain your observations on the matrix. Across which classes do you observe significant confusions?\n",
    "\n",
    "**Features visualization (3 point).** Continue with the best performing model and now take its feature vectors for the *dataitems in the test set*. Project these feature vectors to a 2-dimensional space using the TSNE method.  Using these 2-dimensional vectors, create two plots where the dataitems are shown as points (small circles) on the plots. The plots look exactly the same but only differ in the coloring of the data points. The first plot colors every dataitem with its *true label*, while the second one colors each according to its *predicted label by the model*. Keep in mind to assign the same colors to the classes of the plots, so that the plots are visually comparable. Put these two plots side by side, observe the differences, and compare the results. Report your observations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-optional\"></a><h2 style=\"color:rgb(0,120,170)\">Task C: Linear Model Interpretability (2 extra point)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "Train a logistic regression model on the high-dimensional vectors. Take the coefficient weights, learned by the model, on each dimension (which here corresponds to each token in the dictionary). Separately for each class, study what are the tokens that have the highest contributions/importance for the predictions of the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
